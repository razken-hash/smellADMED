{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-multilearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91386e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "dataset_path = \"./../../../datasets/04_fontana/output/fontana_mld_sc.json\"\n",
    "\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "schema = {\n",
    "    \"id\": []\n",
    "}\n",
    "\n",
    "for i in range(1, 769):\n",
    "    feature = f\"f{i:03}\"\n",
    "    schema[feature] = []\n",
    "\n",
    "schema[\"isFeatureEnvy\"] = []\n",
    "schema[\"isLongMethod\"] = []\n",
    "schema[\"isLongParametersList\"] = []\n",
    "schema[\"isSwitchStatement\"] = []\n",
    "\n",
    "max_length = 512\n",
    "stride = 256\n",
    "\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"microsoft/codebert-base\"  # TODO: Retrieve from config.json\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MODEL_CODE_BERT = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "\n",
    "PADDING = \"max_length\"  # TODO: Retrieve from config.json\n",
    "TRUNCATION = True  # TODO: Retrieve from config.json\n",
    "MAX_LENGTH = 512  # TODO: Retrieve from config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(dataset)):\n",
    "    print(j)\n",
    "    instance = dataset[j]\n",
    "    code = instance[\"source_code\"]\n",
    "\n",
    "    final_embedding = None\n",
    "    method = \"SW\"\n",
    "\n",
    "    if method == \"FIRST\":\n",
    "        # Use a different variable name for tokenized output\n",
    "        tokens = tokenizer(\n",
    "            code,\n",
    "            padding=\"max_length\",\n",
    "            truncation=False,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        # Get the embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract Embeddings (CodeBERT Features)\n",
    "        embedding_vector = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "    if method == \"SW\":\n",
    "        encoding = tokenizer(\n",
    "            code,\n",
    "            add_special_tokens=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=False,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids']\n",
    "\n",
    "        windows = []\n",
    "        # Use a sliding window: note that we subtract one token for the CLS token later.\n",
    "        # Here, we assume we add special tokens manually.\n",
    "        effective_window = max_length - 2  # for CLS and SEP\n",
    "        for i in range(0, len(input_ids), effective_window - stride):\n",
    "            window = input_ids[i: i + effective_window]\n",
    "            # Add special tokens: CLS at beginning, SEP at end.\n",
    "            window = [tokenizer.cls_token_id] + \\\n",
    "                window + [tokenizer.sep_token_id]\n",
    "            # Pad if needed to ensure consistent length (max_length)\n",
    "            if len(window) < max_length:\n",
    "                window = window + [tokenizer.pad_token_id] * \\\n",
    "                    (max_length - len(window))\n",
    "            windows.append(window)\n",
    "            if i + effective_window >= len(input_ids):\n",
    "                break\n",
    "\n",
    "        print(\" ==========> \", len(windows))\n",
    "        # Get the embeddings\n",
    "        allEmbeddings = []\n",
    "        for window in windows:\n",
    "            inputs = {\n",
    "                # add batch dimension\n",
    "                \"input_ids\": torch.tensor([window], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor([[1 if token != tokenizer.pad_token_id else 0 for token in window]], dtype=torch.long)\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            # Extract Embeddings (CodeBERT Features)\n",
    "            embedding_vector = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "            allEmbeddings.append(embedding_vector)\n",
    "\n",
    "        # Compute mean embedding\n",
    "        mean_embedding = torch.mean(torch.stack(allEmbeddings), dim=0)\n",
    "\n",
    "    final_embedding = embedding_vector\n",
    "\n",
    "    schema[\"id\"].append(i + 1)\n",
    "\n",
    "    for k in range(1, 769):\n",
    "        feature = f\"f{k:03}\"\n",
    "        schema[feature].append(final_embedding[k-1].item())\n",
    "\n",
    "    schema[\"isFeatureEnvy\"].append(\n",
    "        1 if \"FeatureEnvy\" in instance[\"code_smells\"] else 0)\n",
    "    schema[\"isLongMethod\"].append(\n",
    "        1 if \"LongMethod\" in instance[\"code_smells\"] else 0)\n",
    "    schema[\"isLongParametersList\"].append(\n",
    "        1 if \"LongParametersList\" in instance[\"code_smells\"] else 0)\n",
    "    schema[\"isSwitchStatement\"].append(\n",
    "        1 if \"SwitchStatement\" in instance[\"code_smells\"] else 0)\n",
    "\n",
    "\n",
    "embedded_smells_datasets_csv = \"./../output/embedded_smells_dataset_fontana.csv\"\n",
    "embedded_smells_datasets_xlsx = \"./../output/embedded_smells_dataset_fontana.xlsx\"\n",
    "\n",
    "embedded_smells_df = pd.DataFrame(schema)\n",
    "embedded_smells_df.to_csv(embedded_smells_datasets_csv, index=False)\n",
    "embedded_smells_df.to_excel(\n",
    "    embedded_smells_datasets_xlsx, index=False, engine=\"openpyxl\")\n",
    "\n",
    "display(embedded_smells_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import product\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "embedded_smells_datasets_csv = \"./../../embeddings/modern_bert/mlcq_class_max_embedding.csv\"\n",
    "\n",
    "\n",
    "# print(features)\n",
    "# print(labels)\n",
    "\n",
    "csv_file = 'mlcq_rf_results.csv'\n",
    "csv_columns = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features',\n",
    "               'train_accuracy', 'val_accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "df = pd.read_csv(embedded_smells_datasets_csv)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # display(df)\n",
    "\n",
    "# god_class_df = df[(df['isGodClass'] == 1) & (df['isDataClass'] == 0)]\n",
    "# both_class_df = df[(df['isGodClass'] == 1) & (df['isDataClass'] == 1)]\n",
    "# data_class_df = df[(df['isGodClass'] == 0) & (df['isDataClass'] == 1)]\n",
    "\n",
    "# # Sample 350 from None\n",
    "# none_sample_df = df[(df['isGodClass'] == 0) & (df['isDataClass'] == 0)].sample(n=len(both_class_df), random_state=42)\n",
    "\n",
    "# # Concatenate all together\n",
    "# df = pd.concat([both_class_df, god_class_df, data_class_df, none_sample_df], ignore_index=True)\n",
    "\n",
    "# # Optional: Shuffle the combined dataset\n",
    "# df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "display(df)\n",
    "\n",
    "features = df.drop(columns=[\"id\", \"isGodClass\", \"isDataClass\"])\n",
    "\n",
    "labels = df[[\"isGodClass\", \"isDataClass\"]]\n",
    "\n",
    "#         # Create and train the model\n",
    "#         # # Train Random Forest with multi-label support\n",
    "features, labels = shuffle(features, labels, random_state=None)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=None)\n",
    "\n",
    "# end = False\n",
    "# a = 1\n",
    "# while not end:\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.3, random_state=None)\n",
    "#     a += 1\n",
    "#     # print(y_train.shape)\n",
    "#     god_class_series = y_train[\"isGodClass\"]\n",
    "#     data_class_series = y_train[\"isDataClass\"]\n",
    "#     # non_class_series = y_train[(y_train[\"isGodClass\"] == 0) & (y_train[\"isDataClass\"] == 0)]\n",
    "\n",
    "\n",
    "#     god_class_count = god_class_series.sum()\n",
    "#     data_class_count = data_class_series.sum()\n",
    "#     non_class_series = y_train[(y_train[\"isGodClass\"] == 0) | (y_train[\"isDataClass\"] == 0)]\n",
    "#     none_class_count = len(non_class_series)\n",
    "\n",
    "#     if 210 <= god_class_count <= 230:\n",
    "#         if 210 <= data_class_count <= 230:\n",
    "#             # if 210 <= none_class_count <= 230:\n",
    "#                 end = True\n",
    "#     else:\n",
    "#         if a % 10 == 0:\n",
    "#             print(a)\n",
    "clear_output()\n",
    "initial_accuracy = 0\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "\n",
    "#     # X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [150],           # Smaller values for faster training\n",
    "        # 'n_estimators': [200, 250, 300],           # Smaller values for faster training\n",
    "        # Keep max_depth small or unlimited\n",
    "        'max_depth': [5, 10, 15, 20],\n",
    "        # Try to avoid splits too small\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        # Leaf nodes with at least 1, 2, or 4 samples\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        # Try different feature selection strategies\n",
    "        'max_features': ['log2']\n",
    "    }\n",
    "\n",
    "#     # Create list of all combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    experiments = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "#     # Loop over each hyperparameter combination\n",
    "    for params in experiments:\n",
    "\n",
    "        base_rf = RandomForestClassifier(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params['max_depth'],\n",
    "            min_samples_split=params['min_samples_split'],\n",
    "            min_samples_leaf=params['min_samples_leaf'],\n",
    "            max_features=params['max_features'],\n",
    "            random_state=42\n",
    "        )\n",
    "        multi_rf = MultiOutputClassifier(base_rf)\n",
    "        multi_rf.fit(X_train, y_train)\n",
    "\n",
    "#         # # Predict and evaluate\n",
    "        y_train_pred = multi_rf.predict(X_train)\n",
    "        y_val_pred = multi_rf.predict(X_val)\n",
    "\n",
    "#         # Metrics\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        precision = precision_score(\n",
    "            y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(\n",
    "            y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        if (val_accuracy > initial_accuracy):\n",
    "            print(val_accuracy)\n",
    "            initial_accuracy = val_accuracy\n",
    "\n",
    "#         # Save result\n",
    "        result_row = {\n",
    "            'n_estimators': params['n_estimators'],\n",
    "            'max_depth': params['max_depth'],\n",
    "            'min_samples_split': params['min_samples_split'],\n",
    "            'min_samples_leaf': params['min_samples_leaf'],\n",
    "            'max_features': params['max_features'],\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "\n",
    "        writer.writerow(result_row)\n",
    "\n",
    "print(f\"Grid search complete. Results saved in {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff660f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
