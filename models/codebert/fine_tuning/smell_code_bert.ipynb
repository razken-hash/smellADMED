{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmellsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512, stride=128):\n",
    "        \"\"\"\n",
    "        texts: List of raw source code strings.\n",
    "        labels: List of labels (e.g., multi-label binary vectors).\n",
    "        tokenizer: The CodeBERT tokenizer.\n",
    "        max_length: Maximum tokens per window.\n",
    "        stride: Overlap between windows.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def sliding_window_tokenize(self, text):\n",
    "        # First, get the full encoding without truncation.\n",
    "        encoding = self.tokenizer(\n",
    "            text, add_special_tokens=False, truncation=False)\n",
    "        input_ids = encoding['input_ids']\n",
    "\n",
    "        windows = []\n",
    "        # Use a sliding window: note that we subtract one token for the CLS token later.\n",
    "        # Here, we assume we add special tokens manually.\n",
    "        effective_window = self.max_length - 2  # for CLS and SEP\n",
    "        for i in range(0, len(input_ids), effective_window - self.stride):\n",
    "            window = input_ids[i: i + effective_window]\n",
    "            # Add special tokens: CLS at beginning, SEP at end.\n",
    "            window = [self.tokenizer.cls_token_id] + \\\n",
    "                window + [self.tokenizer.sep_token_id]\n",
    "            # Pad if needed to ensure consistent length (max_length)\n",
    "            if len(window) < self.max_length:\n",
    "                window = window + [self.tokenizer.pad_token_id] * \\\n",
    "                    (self.max_length - len(window))\n",
    "            windows.append(window)\n",
    "            if i + effective_window >= len(input_ids):\n",
    "                break\n",
    "        return windows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        windows = self.sliding_window_tokenize(text)\n",
    "        \n",
    "        item = {\n",
    "            \"encodings\": windows,\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "        print(\"K\" * 100)\n",
    "        print(item)\n",
    "        print(\"K\" * 100)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmellCodeBERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(SmellCodeBERTClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \"\"\"\n",
    "        input_ids: Tensor of shape (num_windows, max_length)\n",
    "        attention_mask: Tensor of shape (num_windows, max_length)\n",
    "        \"\"\"\n",
    "        outputs = self.roberta(input_ids=input_ids,\n",
    "                               attention_mask=attention_mask)\n",
    "        # Get the CLS token embedding from each window (first token)\n",
    "        # (num_windows, hidden_size)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        # Aggregate using mean pooling over the window dimension:\n",
    "        pooled, _ = torch.max(cls_embeddings, dim=0,\n",
    "                              keepdim=True)  # (1, hidden_size)\n",
    "        logits = self.classifier(pooled)  # (1, num_labels)\n",
    "        output = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.unsqueeze(0))\n",
    "            output[\"loss\"] = loss\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmellTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        # Create a DataLoader for the training dataset with our custom collate function.\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            shuffle=True,\n",
    "            # Use our custom collate function here.\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        # Create a DataLoader for the evaluation dataset with our custom collate function.\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=self.args.per_device_eval_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "    \n",
    "    def custom_collate_fn(batch):\n",
    "        # Print keys for debugging\n",
    "        print(\"Keys in batch sample:\", batch[0].keys())\n",
    "        sample = batch[0]\n",
    "        windows = sample.get(\"encodings\", None)\n",
    "        if windows is None:\n",
    "            raise ValueError(\n",
    "                \"The expected key 'encodings' is missing from the sample.\")\n",
    "        # Shape: (num_windows, max_length)\n",
    "        input_ids = torch.tensor(windows, dtype=torch.long)\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "        labels = torch.tensor(sample[\"labels\"], dtype=torch.float)\n",
    "        item = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask, \n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Print keys for debugging\n",
    "    print(\"Keys in batch sample:\", batch[0].keys())\n",
    "    sample = batch[0]\n",
    "    windows = sample.get(\"encodings\", None)\n",
    "    if windows is None:\n",
    "        raise ValueError(\n",
    "            \"The expected key 'encodings' is missing from the sample.\")\n",
    "    # Shape: (num_windows, max_length)\n",
    "    input_ids = torch.tensor(windows, dtype=torch.long)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    labels = torch.tensor(sample[\"labels\"], dtype=torch.float)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "# # Create dataset\n",
    "dataset = SmellsDataset(texts, lables, tokenizer,\n",
    "                           max_length=512, stride=128)\n",
    "\n",
    "# # Create DataLoader that yields one instance (with its windows) at a time\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=custom_collate_fn)\n",
    "\n",
    "# # Initialize custom model\n",
    "num_labels = 2  # Adjust based on your task\n",
    "model = SmellCodeBERTClassifier(\"microsoft/codebert-base\", num_labels)\n",
    "\n",
    "# # Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    # because each \"sample\" (instance) is processed individually\n",
    "    per_device_train_batch_size=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = SmellTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=custom_collate_fn,  # Our custom collate function\n",
    "    # Optionally, add compute_metrics if you want evaluation metrics.\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to tensor and compute probabilities\n",
    "    logits_tensor = torch.tensor(logits)\n",
    "    probs = torch.sigmoid(logits_tensor)\n",
    "\n",
    "    # If the number of predictions doesn't match the number of labels,\n",
    "    # assume that predictions are per-window and need aggregation.\n",
    "    if probs.shape[0] != np.array(labels).shape[0]:\n",
    "        # Aggregate by taking the mean across the window dimension.\n",
    "        # This results in a single prediction vector per instance.\n",
    "        aggregated_probs = torch.mean(probs, dim=0, keepdim=True)\n",
    "        predictions = (aggregated_probs > 0.5).numpy()[0]\n",
    "    else:\n",
    "        predictions = (probs > 0.5).numpy()[0]\n",
    "\n",
    "    # Ensure labels are numpy arrays\n",
    "    # labels = np.array(labels)\n",
    "\n",
    "    # print(labels)\n",
    "    # print(probs)\n",
    "    # print(predictions)\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1_score(labels, predictions, average=\"micro\"),\n",
    "        \"precision\": precision_score(labels, predictions, average=\"micro\"),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"micro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# Assign the function to your trainer.\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# Then evaluate:\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
